{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful link for italian lenguage support:\n",
    "- http://linguistica.sns.it/CoLFIS/Home_eng.htm   lemmi con frequenze da quotidiani, articoli etc...\n",
    "- http://www.clips.ua.ac.be/pages/pattern-it    pattern ma è in python 2.6 \n",
    "- http://badip.uni-graz.at/it/   altra banca dati di vocabolario (non troppo utile)\n",
    "- nltk tools (vedi link sotto)\n",
    "- http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/   utile per fare il tag delle parole in italiano\n",
    "- https://github.com/miotto/treetagger-python   per usare TreeTagger in python (non riuscito a compilarlo in windows)\n",
    "- morphit (grosso vocabolario italiano mancano le frequenze, vedi link sotto)\n",
    "- https://github.com/napolux/paroleitaliane (altre parole italiane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from math import *\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize: sentences and words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract sentences\n",
    "sents = nltk.sent_tokenize(text)\n",
    "\n",
    "#extract words\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# tagged tokens\n",
    "tagged_tokens = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Come ormai tutti sanno, ogni anno, in gran parte del mondo, si celebra il Giorno della Memoria (27 gennaio), in memoria dei terribili crimini contro l’umanità perpetrati dai Nazisti prima e durante la Seconda Guerra Mondiale.',\n",
       " 'Milioni di ebrei furono deportati nei campi di concentramento e sterminio; i più fortunati riuscirono a nascondersi o fuggire prima.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentences in another language:\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/italian.pickle')\n",
    "\n",
    "italiantext = 'Come ormai tutti sanno, ogni anno, in gran parte del mondo, si celebra il Giorno della Memoria (27 gennaio), in memoria dei terribili crimini contro l’umanità perpetrati dai Nazisti prima e durante la Seconda Guerra Mondiale. Milioni di ebrei furono deportati nei campi di concentramento e sterminio; i più fortunati riuscirono a nascondersi o fuggire prima.'\n",
    "\n",
    "tokenizer.tokenize(italiantext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n"
     ]
    }
   ],
   "source": [
    "# POS tagging\n",
    "nltk.help.upenn_tagset('RB')\n",
    "nltk.help.upenn_tagset('NN.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Non c'è un solo pos tagger...quello di default è il maxent treebank pos tagger, ma ci sono anche\n",
    "# il crf,hmm,brill,tnt\n",
    "\n",
    "# QUALI SONO LE DIFFERENZE??\n",
    "\n",
    "# Trainare un tagger:\n",
    "from nltk.corpus import treebank\n",
    "len(treebank.tagged_sents())\n",
    "train_data = treebank.tagged_sents()[:3000]\n",
    "test_data = treebank.tagged_sents()[3000:]\n",
    "\n",
    "from nltk.tag import tnt\n",
    "tnt_pos_tagger = tnt.TnT()\n",
    "tnt_pos_tagger.train(train_data)\n",
    "tnt_pos_tagger.evaluate(test_data)\n",
    "\n",
    "import pickle\n",
    "f = open(‘tnt_treebank_pos_tagger.pickle’, ‘w’)\n",
    "pickle.dump(tnt_pos_tagger, f)\n",
    "f.close()\n",
    "\n",
    "tnt_tagger.tag(nltk.word_tokenize(“this is a tnt treebank tnt tagger”))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming: \n",
    "### riduzione di una parola alla sua radice o forma base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "# https://tartarus.org/martin/PorterStemmer/\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "# http://snowball.tartarus.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'say'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "porter_stemmer.stem('saying')\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "lancaster_stemmer.stem('saying')\n",
    "\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "snowball_stemmer.stem('saying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parl'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemmer_it = SnowballStemmer('italian')\n",
    "snowball_stemmer_it.stem('parlando')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmization:\n",
    "### riduzione al lemma: considera anche il contesto della parola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://wordnet.princeton.edu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('are')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('is', pos='v')\n",
    "# risulta quindi importante fare pos tagging prima della lemmization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian Tagging and Lemming\n",
    "## Try Morph-it!\n",
    "## http://sslmitdev-online.sslmit.unibo.it/linguistics/morph-it.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STANFORD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_postagger = StanfordPOSTagger('C:\\\\stanford-postagger-full-2014-08-27\\\\models\\\\english-bidirectional-distsim.tagger',\n",
    "                                      'C:\\\\stanford-postagger-full-2014-08-27\\\\stanford-postagger.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('stanford', 'JJ'),\n",
       " ('postagger', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('nltk', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('python', 'NN'),\n",
       " ('users', 'NNS')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_postagger.tag('this is stanford postagger in nltk for python users'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognizer (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-ner.jar jar file at C:\\stanford-ner-2014-08-27\\stanford-ner.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bd33fac66288>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m english_nertagger = StanfordNERTagger('C:\\\\stanford-ner-2014-08-27\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz',\n\u001b[0;32m----> 2\u001b[0;31m                               'C:\\\\stanford-ner-2014-08-27\\\\stanford-ner.jar')\n\u001b[0m",
      "\u001b[0;32m/home/ale/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStanfordNERTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ale/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_JAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0msearchpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 verbose=verbose)\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         self._stanford_model = find_file(model_filename,\n",
      "\u001b[0;32m/home/ale/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    717\u001b[0m         searchpath=(), url=None, verbose=True, is_regex=False):\n\u001b[1;32m    718\u001b[0m     return next(find_jar_iter(name_pattern, path_to_jar, env_vars,\n\u001b[0;32m--> 719\u001b[0;31m                          searchpath, url, verbose, is_regex))\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ale/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             raise LookupError('Could not find %s jar file at %s' %\n\u001b[0;32m--> 635\u001b[0;31m                             (name_pattern, path_to_jar))\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;31m# Check environment variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-ner.jar jar file at C:\\stanford-ner-2014-08-27\\stanford-ner.jar"
     ]
    }
   ],
   "source": [
    "english_nertagger = StanfordNERTagger('C:\\\\stanford-ner-2014-08-27\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz',\n",
    "                              'C:\\\\stanford-ner-2014-08-27\\\\stanford-ner.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_nertagger.tag('Pincopallino is working at StarWars in Montevarchi'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser: analisi logica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_parser = StanfordParser('C:\\\\stanford-parser-full-2014-08-27\\\\stanford-parser.jar',\n",
    "                                'C:\\\\stanford-parser-full-2014-08-27\\\\stanford-parser-3.4.1-models.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analisi = english_parser.parse_sents('Francesco is a good guy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification algorithms:\n",
    "## Assegnazioni di classi o categorie al testo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classici modelli:\n",
    "    - Naive Bayes Model\n",
    "    - Maximum Entropy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "import random\n",
    " \n",
    "names = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])\n",
    " \n",
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets = [(gender_features(n), g) for (n, g) in names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.746\n",
      "Most Informative Features\n",
      "             last_letter = 'k'              male : female =     46.0 : 1.0\n",
      "             last_letter = 'a'            female : male   =     35.9 : 1.0\n",
      "             last_letter = 'p'              male : female =     20.7 : 1.0\n",
      "             last_letter = 'f'              male : female =     16.4 : 1.0\n",
      "             last_letter = 'v'              male : female =     10.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(nb_classifier, test_set))\n",
    "\n",
    "nb_classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.373\n",
      "             2          -0.37252        0.764\n",
      "             3          -0.37211        0.764\n",
      "             4          -0.37187        0.764\n",
      "             5          -0.37171        0.764\n",
      "             6          -0.37159        0.764\n",
      "             7          -0.37150        0.764\n",
      "             8          -0.37144        0.764\n",
      "             9          -0.37138        0.764\n",
      "            10          -0.37134        0.764\n",
      "            11          -0.37130        0.764\n",
      "            12          -0.37127        0.764\n",
      "            13          -0.37124        0.764\n",
      "            14          -0.37122        0.764\n",
      "            15          -0.37120        0.764\n",
      "            16          -0.37118        0.764\n",
      "            17          -0.37116        0.764\n",
      "            18          -0.37115        0.764\n",
      "            19          -0.37114        0.764\n",
      "            20          -0.37113        0.764\n",
      "            21          -0.37112        0.764\n",
      "            22          -0.37111        0.764\n",
      "            23          -0.37110        0.764\n",
      "            24          -0.37109        0.764\n",
      "            25          -0.37108        0.764\n",
      "            26          -0.37107        0.764\n",
      "            27          -0.37107        0.764\n",
      "            28          -0.37106        0.764\n",
      "            29          -0.37106        0.764\n",
      "            30          -0.37105        0.764\n",
      "            31          -0.37105        0.764\n",
      "            32          -0.37104        0.764\n",
      "            33          -0.37104        0.764\n",
      "            34          -0.37103        0.764\n",
      "            35          -0.37103        0.764\n",
      "            36          -0.37103        0.764\n",
      "            37          -0.37102        0.764\n",
      "            38          -0.37102        0.764\n",
      "            39          -0.37102        0.764\n",
      "            40          -0.37101        0.764\n",
      "            41          -0.37101        0.764\n",
      "            42          -0.37101        0.764\n",
      "            43          -0.37101        0.764\n",
      "            44          -0.37100        0.764\n",
      "            45          -0.37100        0.764\n",
      "            46          -0.37100        0.764\n",
      "            47          -0.37100        0.764\n",
      "            48          -0.37099        0.764\n",
      "            49          -0.37099        0.764\n",
      "            50          -0.37099        0.764\n",
      "            51          -0.37099        0.764\n",
      "            52          -0.37099        0.764\n",
      "            53          -0.37098        0.764\n",
      "            54          -0.37098        0.764\n",
      "            55          -0.37098        0.764\n",
      "            56          -0.37098        0.764\n",
      "            57          -0.37098        0.764\n",
      "            58          -0.37098        0.764\n",
      "            59          -0.37098        0.764\n",
      "            60          -0.37097        0.764\n",
      "            61          -0.37097        0.764\n",
      "            62          -0.37097        0.764\n",
      "            63          -0.37097        0.764\n",
      "            64          -0.37097        0.764\n",
      "            65          -0.37097        0.764\n",
      "            66          -0.37097        0.764\n",
      "            67          -0.37097        0.764\n",
      "            68          -0.37097        0.764\n",
      "            69          -0.37096        0.764\n",
      "            70          -0.37096        0.764\n",
      "            71          -0.37096        0.764\n",
      "            72          -0.37096        0.764\n",
      "            73          -0.37096        0.764\n",
      "            74          -0.37096        0.764\n",
      "            75          -0.37096        0.764\n",
      "            76          -0.37096        0.764\n",
      "            77          -0.37096        0.764\n",
      "            78          -0.37096        0.764\n",
      "            79          -0.37096        0.764\n",
      "            80          -0.37095        0.764\n",
      "            81          -0.37095        0.764\n",
      "            82          -0.37095        0.764\n",
      "            83          -0.37095        0.764\n",
      "            84          -0.37095        0.764\n",
      "            85          -0.37095        0.764\n",
      "            86          -0.37095        0.764\n",
      "            87          -0.37095        0.764\n",
      "            88          -0.37095        0.764\n",
      "            89          -0.37095        0.764\n",
      "            90          -0.37095        0.764\n",
      "            91          -0.37095        0.764\n",
      "            92          -0.37095        0.764\n",
      "            93          -0.37095        0.764\n",
      "            94          -0.37095        0.764\n",
      "            95          -0.37095        0.764\n",
      "            96          -0.37094        0.764\n",
      "            97          -0.37094        0.764\n",
      "            98          -0.37094        0.764\n",
      "            99          -0.37094        0.764\n",
      "         Final          -0.37094        0.764\n",
      "0.746\n",
      "   6.644 last_letter==' ' and label is 'female'\n",
      "   6.644 last_letter=='c' and label is 'male'\n",
      "  -4.964 last_letter=='a' and label is 'male'\n",
      "  -4.129 last_letter=='k' and label is 'female'\n",
      "  -3.248 last_letter=='p' and label is 'female'\n"
     ]
    }
   ],
   "source": [
    "# Maxent Classifier\n",
    "\n",
    "me_classifier = nltk.MaxentClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(me_classifier, test_set))\n",
    "\n",
    "me_classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.373\n",
      "             2          -0.61404        0.627\n",
      "             3          -0.59980        0.627\n",
      "             4          -0.58638        0.628\n",
      "             5          -0.57376        0.636\n",
      "             6          -0.56189        0.653\n",
      "             7          -0.55076        0.672\n",
      "      Training stopped: keyboard interrupt\n",
      "         Final          -0.54031        0.688\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(gender_features2(n), g) for (n, g) in names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "\n",
    "nb2_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "me2_classifier = nltk.MaxentClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.766\n",
      "0.714\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(nb2_classifier, test_set))\n",
    "print(nltk.classify.accuracy(me2_classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction with enchant\n",
    "- install via pip install pyenchant\n",
    "- add ita dictionary: sudo apt-get install myspell-it myspell-es\n",
    "- Tutorial at: http://pythonhosted.org/pyenchant/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Enchant: Aspell Provider>, <Enchant: Myspell Provider>, <Enchant: Ispell Provider>, <Enchant: Hspell Provider>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('en', <Enchant: Aspell Provider>),\n",
       " ('en_CA', <Enchant: Aspell Provider>),\n",
       " ('en_GB', <Enchant: Aspell Provider>),\n",
       " ('en_US', <Enchant: Aspell Provider>),\n",
       " ('es_BO', <Enchant: Myspell Provider>),\n",
       " ('es_UY', <Enchant: Myspell Provider>),\n",
       " ('es_GT', <Enchant: Myspell Provider>),\n",
       " ('es_NI', <Enchant: Myspell Provider>),\n",
       " ('es_MX', <Enchant: Myspell Provider>),\n",
       " ('es_AR', <Enchant: Myspell Provider>),\n",
       " ('es_DO', <Enchant: Myspell Provider>),\n",
       " ('es_SV', <Enchant: Myspell Provider>),\n",
       " ('es_EC', <Enchant: Myspell Provider>),\n",
       " ('es_PE', <Enchant: Myspell Provider>),\n",
       " ('es_ES', <Enchant: Myspell Provider>),\n",
       " ('es', <Enchant: Myspell Provider>),\n",
       " ('it_IT', <Enchant: Myspell Provider>),\n",
       " ('es_PA', <Enchant: Myspell Provider>),\n",
       " ('es_CO', <Enchant: Myspell Provider>),\n",
       " ('es_CR', <Enchant: Myspell Provider>),\n",
       " ('es_PR', <Enchant: Myspell Provider>),\n",
       " ('es_CU', <Enchant: Myspell Provider>),\n",
       " ('es_HN', <Enchant: Myspell Provider>),\n",
       " ('es_CL', <Enchant: Myspell Provider>),\n",
       " ('it_CH', <Enchant: Myspell Provider>),\n",
       " ('es_VE', <Enchant: Myspell Provider>),\n",
       " ('es_PY', <Enchant: Myspell Provider>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The underlying programming model provided by the Enchant library is based on the notion of Providers. \n",
    "# A provider is a piece of code that provides spell-checking services which Enchant can use to perform its work. \n",
    "# Different providers exist for performing spellchecking using different frameworks - \n",
    "# for example there is an aspell provider and a MySpell provider.\n",
    "## no need to check brokers while running enchant, this is just a simple check if all is installed\n",
    "b = enchant.Broker()\n",
    "print(b.describe())\n",
    "b.list_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en',\n",
       " 'en_CA',\n",
       " 'en_GB',\n",
       " 'en_US',\n",
       " 'es_BO',\n",
       " 'es_UY',\n",
       " 'es_GT',\n",
       " 'es_NI',\n",
       " 'es_MX',\n",
       " 'es_AR',\n",
       " 'es_DO',\n",
       " 'es_SV',\n",
       " 'es_EC',\n",
       " 'es_PE',\n",
       " 'es_ES',\n",
       " 'es',\n",
       " 'it_IT',\n",
       " 'es_PA',\n",
       " 'es_CO',\n",
       " 'es_CR',\n",
       " 'es_PR',\n",
       " 'es_CU',\n",
       " 'es_HN',\n",
       " 'es_CL',\n",
       " 'it_CH',\n",
       " 'es_VE',\n",
       " 'es_PY']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enchant.list_languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = enchant.Dict(\"it_IT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.check('Giulia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.check('pappapero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potrei ma', 'potrei-ma', 'potrei', 'impomatare']\n",
      "['marame', 'marea', 'maremma', 'Carema', 'ma rema', 'ma-rema', 'mare ma', 'mare-ma', 'Maremma', 'remare', 'remar', 'mare']\n",
      "[]\n",
      "['vanno', 'vano']\n",
      "['duellatole']\n"
     ]
    }
   ],
   "source": [
    "print( d.suggest(\"potreima\") )\n",
    "print( d.suggest(\"marema\") )\n",
    "print( d.suggest(\"se metto troppe parole lo impallo\") )\n",
    "print( d.suggest(\"van no\") )\n",
    "print( d.suggest(\"due parole\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add your own dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dict objects can also be used to check words against a custom list of correctly-spelled words \n",
    "# known as a Personal Word List. This is simply a file listing the words to be considered, one word per line. \n",
    "# The following example creates a Dict object for the personal word list stored in “mywords.txt”:\n",
    "pwl = enchant.request_pwl_dict(\"../Data_nlp/mywords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwl.check('pappapero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cittino', 'cittina']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwl.suggest('cittin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwl.check('altro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PyEnchant also provides the class DictWithPWL which can be used to combine a language dictionary \n",
    "# and a personal word list file:\n",
    "d2 = enchant.DictWithPWL(\"it_IT\", \"../Data_nlp/mywords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2.check('altro') & d2.check('pappapero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cittadino', 'cittino', 'cittina']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2.suggest('cittin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.4 ms ± 423 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "d2.suggest('poliza')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check entire phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from enchant.checker import SpellChecker\n",
    "chkr = SpellChecker(\"it_IT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picclo\n",
      "['picco', 'piccolo', 'picciolo', 'epiciclo', 'ciclopico']\n",
      "esmpio\n",
      "['espio', 'empio', 'esempio']\n"
     ]
    }
   ],
   "source": [
    "chkr.set_text(\"questo è un picclo esmpio per dire cm funziona\")\n",
    "for err in chkr:\n",
    "    print(err.word)\n",
    "    print(chkr.suggest(err.word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esmpio 19\n"
     ]
    }
   ],
   "source": [
    "print(chkr.word, chkr.wordpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'questo è un picclo pippo per dire cm funziona'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chkr.replace('pippo')\n",
    "chkr.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenization\n",
    "As explained above, the module enchant.tokenize provides the ability to split text into its component words. The current implementation is based only on the rules for the English language, and so might not be completely suitable for your language of choice. Fortunately, it is straightforward to extend the functionality of this module.\n",
    "\n",
    "To implement a new tokenization routine for the language TAG, simply create a class/function “tokenize” within the module “enchant.tokenize.TAG”. This function will automatically be detected by the module’s get_tokenizer function and used when appropriate. The easiest way to accomplish this is to copy the module “enchant.tokenize.en” and modify it to suit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 0), ('is', 5), ('some', 8), ('simple', 13), ('text', 20)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enchant.tokenize import get_tokenizer\n",
    "tknzr = get_tokenizer(\"en_US\") # not tak for it_IT up to now\n",
    "[w for w in tknzr(\"this is some simple text\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 0),\n",
       " ('is', 5),\n",
       " ('span', 9),\n",
       " ('class', 14),\n",
       " ('important', 21),\n",
       " ('really', 32),\n",
       " ('important', 39),\n",
       " ('span', 50),\n",
       " ('text', 56)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enchant.tokenize import get_tokenizer, HTMLChunker\n",
    "tknzr = get_tokenizer(\"en_US\")\n",
    "[w for w in tknzr(\"this is <span class='important'>really important</span> text\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 0), ('is', 5), ('really', 32), ('important', 39), ('text', 56)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = get_tokenizer(\"en_US\",chunkers=(HTMLChunker,))\n",
    "[w for w in tknzr(\"this is <span class='important'>really important</span> text\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('send', 0),\n",
       " ('an', 5),\n",
       " ('email', 8),\n",
       " ('to', 14),\n",
       " ('fake', 17),\n",
       " ('example', 22),\n",
       " ('com', 30),\n",
       " ('please', 34)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enchant.tokenize import get_tokenizer, EmailFilter\n",
    "tknzr = get_tokenizer(\"en_US\")\n",
    "[w for w in tknzr(\"send an email to fake@example.com please\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('send', 0), ('an', 5), ('email', 8), ('to', 14), ('please', 34)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = get_tokenizer(\"en_US\", filters = [EmailFilter])\n",
    "[w for w in tknzr(\"send an email to fake@example.com please\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other modules:\n",
    "- CmdLineChecker\n",
    "\n",
    "The module enchant.checker.CmdLineChecker provides the class CmdLineChecker which can be used to interactively check the spelling of some text. It uses standard input and standard output to interact with the user through a command-line interface. The code below shows how to create and use this class from within a python application, along with a short sample checking session:\n",
    "\n",
    "- wxSpellCheckerDialog\n",
    "\n",
    "The module enchant.checker.wxSpellCheckerDialog provides the class wxSpellCheckerDialog which can be used to interactively check the spelling of some text. The code below shows how to create and use such a dialog from within a wxPython application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Word2vec\n",
    "- pip install gensim\n",
    "- pip install pyemd\n",
    "- https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim, logging\n",
    "from  gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    '../Data_nlp/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brian'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"breakfast brian dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evaluate_word_pairs() missing 1 required positional argument: 'pairs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8fd5a98fc876>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# give text with w1 w2 your_distance to check if model and w1-w2 have give the same distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_word_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: evaluate_word_pairs() missing 1 required positional argument: 'pairs'"
     ]
    }
   ],
   "source": [
    "# give text with w1 w2 your_distance to check if model and w1-w2 have give the same distance\n",
    "model.evaluate_word_pairs() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check accuracy against a premade grouped words\n",
    "questions_words = model.accuracy('../Data_nlp/word2vec/trunk/questions-words.txt')\n",
    "phrases_words = model.accuracy('../Data_nlp/word2vec/trunk/questions-phrases.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BOY', 'GIRL', 'STEPFATHER', 'STEPMOTHER'),\n",
       " ('BROTHER', 'SISTER', 'STEPFATHER', 'STEPMOTHER'),\n",
       " ('BROTHERS', 'SISTERS', 'STEPFATHER', 'STEPMOTHER'),\n",
       " ('DAD', 'MOM', 'HUSBAND', 'WIFE'),\n",
       " ('DAD', 'MOM', 'STEPFATHER', 'STEPMOTHER'),\n",
       " ('GRANDFATHER', 'GRANDMOTHER', 'HUSBAND', 'WIFE'),\n",
       " ('GRANDFATHER', 'GRANDMOTHER', 'STEPFATHER', 'STEPMOTHER'),\n",
       " ('GRANDSON', 'GRANDDAUGHTER', 'STEPFATHER', 'STEPMOTHER'),\n",
       " ('GROOM', 'BRIDE', 'NEPHEW', 'NIECE'),\n",
       " ('GROOM', 'BRIDE', 'STEPFATHER', 'STEPMOTHER'),\n",
       " ('GROOM', 'BRIDE', 'UNCLE', 'AUNT'),\n",
       " ('GROOM', 'BRIDE', 'BROTHER', 'SISTER'),\n",
       " ('GROOM', 'BRIDE', 'BROTHERS', 'SISTERS'),\n",
       " ('HE', 'SHE', 'HUSBAND', 'WIFE'),\n",
       " ('HE', 'SHE', 'STEPFATHER', 'STEPMOTHER'),\n",
       " ('HIS', 'HER', 'HUSBAND', 'WIFE'),\n",
       " ('HIS', 'HER', 'STEPFATHER', 'STEPMOTHER'),\n",
       " ('HUSBAND', 'WIFE', 'BROTHER', 'SISTER'),\n",
       " ('HUSBAND', 'WIFE', 'DAD', 'MOM'),\n",
       " ('HUSBAND', 'WIFE', 'FATHER', 'MOTHER'),\n",
       " ('HUSBAND', 'WIFE', 'GRANDFATHER', 'GRANDMOTHER'),\n",
       " ('HUSBAND', 'WIFE', 'HE', 'SHE'),\n",
       " ('HUSBAND', 'WIFE', 'HIS', 'HER'),\n",
       " ('KING', 'QUEEN', 'STEPFATHER', 'STEPMOTHER'),\n",
       " ('MAN', 'WOMAN', 'STEPFATHER', 'STEPMOTHER'),\n",
       " ('MAN', 'WOMAN', 'HUSBAND', 'WIFE'),\n",
       " ('NEPHEW', 'NIECE', 'STEPFATHER', 'STEPMOTHER'),\n",
       " ('PRINCE', 'PRINCESS', 'STEPFATHER', 'STEPMOTHER'),\n",
       " ('PRINCE', 'PRINCESS', 'DAD', 'MOM'),\n",
       " ('PRINCE', 'PRINCESS', 'HUSBAND', 'WIFE'),\n",
       " ('SON', 'DAUGHTER', 'STEPFATHER', 'STEPMOTHER'),\n",
       " ('SONS', 'DAUGHTERS', 'STEPFATHER', 'STEPMOTHER'),\n",
       " ('STEPFATHER', 'STEPMOTHER', 'GRANDFATHER', 'GRANDMOTHER'),\n",
       " ('UNCLE', 'AUNT', 'STEPFATHER', 'STEPMOTHER')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_words[4]['incorrect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.660332138317\n",
      "0.458195260447\n",
      "0.177056094484\n",
      "0.760945708978\n"
     ]
    }
   ],
   "source": [
    "print( model.n_similarity(['pasta'], ['spaghetti']) )\n",
    "print( model.n_similarity(['pasta'], ['tomato']) )\n",
    "print( model.n_similarity(['pasta'], ['car']) )\n",
    "print( model.n_similarity(['cat'], ['dog']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('welcome', 1.0),\n",
       " ('welcomed', 0.7077772617340088),\n",
       " ('welcoming', 0.7071465849876404),\n",
       " ('welcomes', 0.6647579669952393),\n",
       " ('warmly_welcomed', 0.6219103336334229),\n",
       " ('warmly_welcome', 0.5892778038978577),\n",
       " ('Welcoming', 0.5658251047134399),\n",
       " ('greatly_appreciated', 0.5299198627471924),\n",
       " ('warmly_welcomes', 0.521955132484436),\n",
       " ('invite', 0.5170012712478638)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_vector( model.word_vec('welcome') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('welcomed', 0.7077772617340088),\n",
       " ('welcoming', 0.7071465849876404),\n",
       " ('welcomes', 0.6647579669952393),\n",
       " ('warmly_welcomed', 0.6219103336334229),\n",
       " ('warmly_welcome', 0.5892777442932129),\n",
       " ('Welcoming', 0.5658251047134399),\n",
       " ('greatly_appreciated', 0.5299198627471924),\n",
       " ('warmly_welcomes', 0.521955132484436),\n",
       " ('invite', 0.517001211643219),\n",
       " ('delighted', 0.5136862397193909)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_word('welcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00704956, -0.07324219,  0.171875  ,  0.02258301, -0.1328125 ,\n",
       "        0.19824219,  0.11279297, -0.10791016,  0.07177734,  0.02087402,\n",
       "       -0.12304688, -0.05908203,  0.10107422,  0.01074219,  0.14355469,\n",
       "        0.25976562, -0.03637695,  0.18554688, -0.07861328, -0.02270508,\n",
       "       -0.12060547,  0.17773438,  0.04956055,  0.01721191,  0.07958984,\n",
       "       -0.0456543 , -0.18847656,  0.18945312, -0.02319336,  0.06298828,\n",
       "        0.09765625, -0.01904297, -0.07910156,  0.15234375,  0.17382812,\n",
       "        0.1015625 , -0.16308594,  0.11474609,  0.10058594, -0.09277344,\n",
       "        0.109375  ,  0.05883789, -0.02160645,  0.06347656,  0.04199219,\n",
       "       -0.0088501 ,  0.03222656,  0.10644531,  0.06445312, -0.11865234,\n",
       "        0.03051758,  0.06689453,  0.12207031, -0.08300781,  0.171875  ,\n",
       "        0.07861328,  0.09521484, -0.00778198,  0.02319336,  0.0234375 ,\n",
       "       -0.0168457 ,  0.15527344, -0.10986328, -0.17675781, -0.11621094,\n",
       "        0.0234375 , -0.01062012,  0.05273438, -0.13378906,  0.07958984,\n",
       "        0.07373047,  0.04394531,  0.11523438, -0.02062988,  0.07470703,\n",
       "       -0.01153564,  0.08056641,  0.04174805,  0.08007812,  0.3515625 ,\n",
       "        0.09667969, -0.21289062,  0.16503906, -0.078125  ,  0.06982422,\n",
       "       -0.00139618, -0.09130859,  0.12988281,  0.25195312, -0.01611328,\n",
       "        0.09326172, -0.14648438, -0.00151062, -0.15136719, -0.02685547,\n",
       "       -0.15722656,  0.02636719,  0.0859375 ,  0.07177734,  0.07714844,\n",
       "       -0.0390625 ,  0.05444336, -0.12792969,  0.09130859, -0.18457031,\n",
       "       -0.03759766, -0.0279541 , -0.08984375, -0.11669922, -0.09863281,\n",
       "        0.0480957 , -0.16210938, -0.10888672,  0.08496094, -0.0456543 ,\n",
       "        0.15820312, -0.03808594, -0.08203125,  0.203125  ,  0.08642578,\n",
       "        0.06933594,  0.03222656, -0.16015625,  0.09472656, -0.0246582 ,\n",
       "        0.05419922,  0.0279541 ,  0.04492188,  0.16992188,  0.07275391,\n",
       "       -0.03637695, -0.01025391, -0.01708984, -0.10742188, -0.0007019 ,\n",
       "       -0.07373047,  0.25390625,  0.05664062,  0.03515625, -0.00860596,\n",
       "        0.18554688,  0.02148438,  0.26367188, -0.02380371, -0.09912109,\n",
       "       -0.04125977, -0.06933594, -0.11376953,  0.05004883, -0.05883789,\n",
       "        0.04614258,  0.08740234,  0.10546875,  0.10644531,  0.0279541 ,\n",
       "        0.09472656,  0.11621094, -0.17285156, -0.03491211, -0.20800781,\n",
       "        0.05957031,  0.10400391, -0.00179291,  0.05859375, -0.02978516,\n",
       "       -0.03759766,  0.04858398, -0.06396484,  0.07958984,  0.06933594,\n",
       "       -0.10498047, -0.14453125,  0.04345703, -0.06884766, -0.03564453,\n",
       "       -0.01171875,  0.01367188, -0.06591797,  0.11914062,  0.03125   ,\n",
       "       -0.04638672, -0.00196838,  0.00735474, -0.05664062,  0.02783203,\n",
       "        0.08251953, -0.01348877,  0.07177734,  0.14453125,  0.12792969,\n",
       "        0.04223633,  0.14160156, -0.01806641,  0.02160645, -0.09179688,\n",
       "        0.13378906, -0.1953125 , -0.05029297, -0.0378418 , -0.09619141,\n",
       "        0.10302734, -0.10693359, -0.14746094,  0.09960938, -0.23046875,\n",
       "        0.22753906, -0.07519531,  0.06494141,  0.09179688,  0.046875  ,\n",
       "        0.06298828,  0.06982422,  0.04614258,  0.09716797, -0.20214844,\n",
       "        0.19921875,  0.18652344, -0.11962891, -0.14257812,  0.15039062,\n",
       "       -0.03369141, -0.14550781, -0.00069046, -0.07324219,  0.13378906,\n",
       "        0.03564453, -0.02294922,  0.02770996, -0.07910156,  0.20703125,\n",
       "       -0.08349609, -0.04956055,  0.03149414,  0.1484375 ,  0.05566406,\n",
       "       -0.04492188, -0.07958984,  0.00476074, -0.02075195,  0.06005859,\n",
       "        0.00476074,  0.01116943,  0.17285156, -0.13476562,  0.03076172,\n",
       "       -0.07958984,  0.09033203,  0.06103516,  0.07714844, -0.05029297,\n",
       "       -0.09228516, -0.26757812,  0.10791016,  0.0859375 ,  0.06298828,\n",
       "        0.10791016, -0.0267334 ,  0.10205078, -0.12060547,  0.05297852,\n",
       "        0.09472656, -0.16503906,  0.04418945,  0.07226562,  0.04125977,\n",
       "        0.42578125, -0.10302734, -0.16015625, -0.09033203, -0.06396484,\n",
       "       -0.0480957 ,  0.14453125,  0.06542969,  0.04931641,  0.05419922,\n",
       "        0.13574219, -0.01928711, -0.21582031, -0.07421875, -0.14648438,\n",
       "        0.01147461, -0.16503906, -0.10498047,  0.00320435,  0.13476562,\n",
       "       -0.00396729, -0.10351562, -0.13964844,  0.10449219, -0.01257324,\n",
       "       -0.23339844, -0.03637695, -0.09375   ,  0.18261719,  0.02709961,\n",
       "        0.12792969, -0.02478027,  0.01123047,  0.1640625 ,  0.10693359], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0[4,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.index2word[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00704956, -0.07324219,  0.171875  ,  0.02258301, -0.1328125 ,\n",
       "        0.19824219,  0.11279297, -0.10791016,  0.07177734,  0.02087402,\n",
       "       -0.12304688, -0.05908203,  0.10107422,  0.01074219,  0.14355469,\n",
       "        0.25976562, -0.03637695,  0.18554688, -0.07861328, -0.02270508,\n",
       "       -0.12060547,  0.17773438,  0.04956055,  0.01721191,  0.07958984,\n",
       "       -0.0456543 , -0.18847656,  0.18945312, -0.02319336,  0.06298828,\n",
       "        0.09765625, -0.01904297, -0.07910156,  0.15234375,  0.17382812,\n",
       "        0.1015625 , -0.16308594,  0.11474609,  0.10058594, -0.09277344,\n",
       "        0.109375  ,  0.05883789, -0.02160645,  0.06347656,  0.04199219,\n",
       "       -0.0088501 ,  0.03222656,  0.10644531,  0.06445312, -0.11865234,\n",
       "        0.03051758,  0.06689453,  0.12207031, -0.08300781,  0.171875  ,\n",
       "        0.07861328,  0.09521484, -0.00778198,  0.02319336,  0.0234375 ,\n",
       "       -0.0168457 ,  0.15527344, -0.10986328, -0.17675781, -0.11621094,\n",
       "        0.0234375 , -0.01062012,  0.05273438, -0.13378906,  0.07958984,\n",
       "        0.07373047,  0.04394531,  0.11523438, -0.02062988,  0.07470703,\n",
       "       -0.01153564,  0.08056641,  0.04174805,  0.08007812,  0.3515625 ,\n",
       "        0.09667969, -0.21289062,  0.16503906, -0.078125  ,  0.06982422,\n",
       "       -0.00139618, -0.09130859,  0.12988281,  0.25195312, -0.01611328,\n",
       "        0.09326172, -0.14648438, -0.00151062, -0.15136719, -0.02685547,\n",
       "       -0.15722656,  0.02636719,  0.0859375 ,  0.07177734,  0.07714844,\n",
       "       -0.0390625 ,  0.05444336, -0.12792969,  0.09130859, -0.18457031,\n",
       "       -0.03759766, -0.0279541 , -0.08984375, -0.11669922, -0.09863281,\n",
       "        0.0480957 , -0.16210938, -0.10888672,  0.08496094, -0.0456543 ,\n",
       "        0.15820312, -0.03808594, -0.08203125,  0.203125  ,  0.08642578,\n",
       "        0.06933594,  0.03222656, -0.16015625,  0.09472656, -0.0246582 ,\n",
       "        0.05419922,  0.0279541 ,  0.04492188,  0.16992188,  0.07275391,\n",
       "       -0.03637695, -0.01025391, -0.01708984, -0.10742188, -0.0007019 ,\n",
       "       -0.07373047,  0.25390625,  0.05664062,  0.03515625, -0.00860596,\n",
       "        0.18554688,  0.02148438,  0.26367188, -0.02380371, -0.09912109,\n",
       "       -0.04125977, -0.06933594, -0.11376953,  0.05004883, -0.05883789,\n",
       "        0.04614258,  0.08740234,  0.10546875,  0.10644531,  0.0279541 ,\n",
       "        0.09472656,  0.11621094, -0.17285156, -0.03491211, -0.20800781,\n",
       "        0.05957031,  0.10400391, -0.00179291,  0.05859375, -0.02978516,\n",
       "       -0.03759766,  0.04858398, -0.06396484,  0.07958984,  0.06933594,\n",
       "       -0.10498047, -0.14453125,  0.04345703, -0.06884766, -0.03564453,\n",
       "       -0.01171875,  0.01367188, -0.06591797,  0.11914062,  0.03125   ,\n",
       "       -0.04638672, -0.00196838,  0.00735474, -0.05664062,  0.02783203,\n",
       "        0.08251953, -0.01348877,  0.07177734,  0.14453125,  0.12792969,\n",
       "        0.04223633,  0.14160156, -0.01806641,  0.02160645, -0.09179688,\n",
       "        0.13378906, -0.1953125 , -0.05029297, -0.0378418 , -0.09619141,\n",
       "        0.10302734, -0.10693359, -0.14746094,  0.09960938, -0.23046875,\n",
       "        0.22753906, -0.07519531,  0.06494141,  0.09179688,  0.046875  ,\n",
       "        0.06298828,  0.06982422,  0.04614258,  0.09716797, -0.20214844,\n",
       "        0.19921875,  0.18652344, -0.11962891, -0.14257812,  0.15039062,\n",
       "       -0.03369141, -0.14550781, -0.00069046, -0.07324219,  0.13378906,\n",
       "        0.03564453, -0.02294922,  0.02770996, -0.07910156,  0.20703125,\n",
       "       -0.08349609, -0.04956055,  0.03149414,  0.1484375 ,  0.05566406,\n",
       "       -0.04492188, -0.07958984,  0.00476074, -0.02075195,  0.06005859,\n",
       "        0.00476074,  0.01116943,  0.17285156, -0.13476562,  0.03076172,\n",
       "       -0.07958984,  0.09033203,  0.06103516,  0.07714844, -0.05029297,\n",
       "       -0.09228516, -0.26757812,  0.10791016,  0.0859375 ,  0.06298828,\n",
       "        0.10791016, -0.0267334 ,  0.10205078, -0.12060547,  0.05297852,\n",
       "        0.09472656, -0.16503906,  0.04418945,  0.07226562,  0.04125977,\n",
       "        0.42578125, -0.10302734, -0.16015625, -0.09033203, -0.06396484,\n",
       "       -0.0480957 ,  0.14453125,  0.06542969,  0.04931641,  0.05419922,\n",
       "        0.13574219, -0.01928711, -0.21582031, -0.07421875, -0.14648438,\n",
       "        0.01147461, -0.16503906, -0.10498047,  0.00320435,  0.13476562,\n",
       "       -0.00396729, -0.10351562, -0.13964844,  0.10449219, -0.01257324,\n",
       "       -0.23339844, -0.03637695, -0.09375   ,  0.18261719,  0.02709961,\n",
       "        0.12792969, -0.02478027,  0.01123047,  0.1640625 ,  0.10693359], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_vec('is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00374603, -0.03891977,  0.09133173,  0.01200026, -0.07057451,\n",
       "        0.10534285,  0.05993645, -0.0573418 ,  0.03814138,  0.01109213,\n",
       "       -0.06538521, -0.03139528,  0.05370928,  0.00570823,  0.07628275,\n",
       "        0.13803546, -0.01933015,  0.09859675, -0.04177389, -0.01206513,\n",
       "       -0.06408789,  0.09444531,  0.02633571,  0.00914615,  0.04229282,\n",
       "       -0.02425999, -0.10015354,  0.10067248, -0.01232459,  0.033471  ,\n",
       "        0.05189303, -0.01011914, -0.04203335,  0.08095312,  0.09236959,\n",
       "        0.05396875, -0.08666135,  0.06097431,  0.05344982, -0.04929838,\n",
       "        0.05812019,  0.03126555, -0.01148133,  0.03373047,  0.022314  ,\n",
       "       -0.00470281,  0.0171247 ,  0.0565634 ,  0.0342494 , -0.06305003,\n",
       "        0.01621657,  0.03554672,  0.06486628, -0.04410907,  0.09133173,\n",
       "        0.04177389,  0.0505957 , -0.00413523,  0.01232459,  0.01245433,\n",
       "       -0.00895155,  0.08250991, -0.05837966, -0.09392638, -0.0617527 ,\n",
       "        0.01245433, -0.00564337,  0.02802224, -0.07109345,  0.04229282,\n",
       "        0.03917924,  0.02335186,  0.06123377, -0.0109624 ,  0.03969816,\n",
       "       -0.00612986,  0.04281175,  0.02218427,  0.04255228,  0.1868149 ,\n",
       "        0.0513741 , -0.1131268 ,  0.08769922, -0.04151442,  0.03710352,\n",
       "       -0.00074191, -0.04851998,  0.06901773,  0.13388401, -0.00856235,\n",
       "        0.04955784, -0.07783955, -0.00080272, -0.0804342 , -0.01427058,\n",
       "       -0.08354778,  0.01401112,  0.04566586,  0.03814138,  0.04099549,\n",
       "       -0.02075721,  0.02893036, -0.06797986,  0.04851998, -0.09807783,\n",
       "       -0.01997882, -0.01485438, -0.04774158, -0.06201217, -0.05241196,\n",
       "        0.02555732, -0.08614243, -0.05786072,  0.04514693, -0.02425999,\n",
       "        0.0840667 , -0.02023828, -0.04359014,  0.1079375 ,  0.04592533,\n",
       "        0.03684405,  0.0171247 , -0.08510457,  0.05033624, -0.01310299,\n",
       "        0.02880063,  0.01485438,  0.02387079,  0.09029387,  0.03866031,\n",
       "       -0.01933015, -0.00544877, -0.00908128, -0.05708233, -0.00037298,\n",
       "       -0.03917924,  0.13492188,  0.03009796,  0.01868149, -0.00457307,\n",
       "        0.09859675,  0.01141647,  0.14011118, -0.01264893, -0.05267143,\n",
       "       -0.0219248 , -0.03684405, -0.06045538,  0.02659518, -0.03126555,\n",
       "        0.02451946,  0.04644426,  0.05604447,  0.0565634 ,  0.01485438,\n",
       "        0.05033624,  0.0617527 , -0.09185066, -0.01855176, -0.11053215,\n",
       "        0.03165475,  0.05526607, -0.00095272,  0.03113582, -0.01582737,\n",
       "       -0.01997882,  0.02581678, -0.03398993,  0.04229282,  0.03684405,\n",
       "       -0.055785  , -0.07680168,  0.0230924 , -0.03658459, -0.01894096,\n",
       "       -0.00622716,  0.00726502, -0.03502779,  0.06330949,  0.01660577,\n",
       "       -0.02464919, -0.00104597,  0.00390819, -0.03009796,  0.01478951,\n",
       "        0.04384961, -0.00716772,  0.03814138,  0.07680168,  0.06797986,\n",
       "        0.02244373,  0.07524489, -0.00960021,  0.01148133, -0.04877945,\n",
       "        0.07109345, -0.10378606, -0.02672491, -0.02010855, -0.05111463,\n",
       "        0.05474715, -0.05682287, -0.07835847,  0.05293089, -0.12246755,\n",
       "        0.12091076, -0.03995763,  0.03450887,  0.04877945,  0.02490865,\n",
       "        0.033471  ,  0.03710352,  0.02451946,  0.05163356, -0.10741857,\n",
       "        0.10586178,  0.09911568, -0.06356896, -0.07576382,  0.07991526,\n",
       "       -0.0179031 , -0.07732061, -0.0003669 , -0.03891977,  0.07109345,\n",
       "        0.01894096, -0.01219486,  0.01472465, -0.04203335,  0.11001322,\n",
       "       -0.04436854, -0.02633571,  0.0167355 ,  0.0788774 ,  0.02957903,\n",
       "       -0.02387079, -0.04229282,  0.00252979, -0.01102727,  0.03191421,\n",
       "        0.00252979,  0.00593527,  0.09185066, -0.07161238,  0.0163463 ,\n",
       "       -0.04229282,  0.04800105,  0.03243314,  0.04099549, -0.02672491,\n",
       "       -0.04903891, -0.1421869 ,  0.0573418 ,  0.04566586,  0.033471  ,\n",
       "        0.0573418 , -0.01420572,  0.05422821, -0.06408789,  0.02815197,\n",
       "        0.05033624, -0.08769922,  0.02348159,  0.03840084,  0.0219248 ,\n",
       "        0.2262536 , -0.05474715, -0.08510457, -0.04800105, -0.03398993,\n",
       "       -0.02555732,  0.07680168,  0.03476833,  0.02620598,  0.02880063,\n",
       "        0.07213131, -0.01024887, -0.11468359, -0.0394387 , -0.07783955,\n",
       "        0.00609743, -0.08769922, -0.055785  ,  0.00170274,  0.07161238,\n",
       "       -0.00210815, -0.05500661, -0.07420703,  0.05552554, -0.00668123,\n",
       "       -0.12402434, -0.01933015, -0.04981731,  0.09703996,  0.01440032,\n",
       "        0.06797986, -0.01316786,  0.0059677 ,  0.08718029,  0.05682287], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0norm[4,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Goofy', 0.796820342540741),\n",
       " ('Minni', 0.7049012184143066),\n",
       " ('Mickey_Minnie_Goofy', 0.5468583703041077),\n",
       " ('Mickey_Goofy', 0.5395780205726624),\n",
       " ('Pluto_Goofy', 0.5347572565078735),\n",
       " ('Mickey_Minnie', 0.5343326330184937),\n",
       " ('Daisy_Duck', 0.5236194729804993),\n",
       " ('Sora_Donald', 0.5230178236961365),\n",
       " ('Mickey_Mouse_Goofy', 0.5048299431800842),\n",
       " ('nephews_Huey_Dewey', 0.5034050345420837)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "model.similar_by_vector( (model.word_vec('Goofy') + model.word_vec('Minni'))/2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3741233214730024\n"
     ]
    }
   ],
   "source": [
    "import pyemd\n",
    "# This method only works if `pyemd` is installed (can be installed via pip, but requires a C compiler).\n",
    "\n",
    "sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
    "sentence_president = 'The president greets the press in Chicago'.lower().split()\n",
    "\n",
    "# Remove their stopwords.\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "sentence_obama = [w for w in sentence_obama if w not in stopwords]\n",
    "sentence_president = [w for w in sentence_president if w not in stopwords]\n",
    "\n",
    "# Compute WMD.\n",
    "distance = model.wmdistance(sentence_obama, sentence_president)\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def sentence_distance(s1, s2):\n",
    "    sentence_obama = [w for w in s1.split() if w not in stopwords]\n",
    "    sentence_president = [w for w in s2.split() if w not in stopwords]\n",
    "    print(sentence_obama, sentence_president, sep='\\t')\n",
    "    print(model.wmdistance(sentence_obama, sentence_president), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'run', 'every', 'day', 'morning']\t['I', 'like', 'football']\n",
      "2.3376889762187165\n",
      "\n",
      "['I', 'run', 'every', 'day', 'morning']\t['I', 'run', 'since', 'I', 'born']\n",
      "1.820895138922882\n",
      "\n",
      "['I', 'run', 'every', 'day', 'morning']\t['idiot']\n",
      "3.3750919594666007\n",
      "\n",
      "['I', 'run', 'every', 'day', 'morning']\t['Are', 'idiot?']\n",
      "3.976704031329918\n",
      "\n",
      "['I', 'run', 'every', 'day', 'morning']\t['Is', 'possible', 'die?']\n",
      "3.1081644990045545\n",
      "\n",
      "['I', 'run', 'every', 'day', 'morning']\t['Is', 'possible', 'die']\n",
      "3.1858849239788394\n",
      "\n",
      "['I', 'run', 'every', 'day', 'morning']\t['I', 'run', 'every', 'day']\n",
      "0.5563409008898735\n",
      "\n",
      "['I', 'run', 'every', 'day', 'morning']\t['I', 'eat', 'every', 'day']\n",
      "1.2005782773353697\n",
      "\n",
      "['I', 'run', 'every', 'day', 'morning']\t['I', 'breakfast', 'morning']\n",
      "1.711929159530717\n",
      "\n",
      "['I', 'run', 'every', 'day', 'morning']\t['I', 'breakfast', 'every', 'day', 'morning']\n",
      "0.6631782969713211\n",
      "\n",
      "['I', 'run', 'every', 'day', 'morning']\t['Each', 'day', 'I', 'run']\n",
      "1.1626442679190636\n",
      "\n",
      "['I', 'run', 'every', 'day', 'morning']\t['I', 'run', 'every', 'day', 'morning']\n",
      "0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_distance('I run every day in the morning', 'I like football')\n",
    "sentence_distance('I run every day in the morning', 'I run since I was born')\n",
    "sentence_distance('I run every day in the morning', 'you are idiot')\n",
    "sentence_distance('I run every day in the morning', 'Are you idiot?')\n",
    "sentence_distance('I run every day in the morning', 'Is it possible to die?')\n",
    "sentence_distance('I run every day in the morning', 'Is it possible to die')\n",
    "sentence_distance('I run every day in the morning', 'I run every day')\n",
    "sentence_distance('I run every day in the morning', 'I eat every day')\n",
    "sentence_distance('I run every day in the morning', 'I have breakfast in the morning')\n",
    "sentence_distance('I run every day in the morning', 'I have breakfast every day in the morning')\n",
    "sentence_distance('I run every day in the morning', 'Each day I run')\n",
    "sentence_distance('I run every day in the morning', 'I run every day in the morning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'run', 'every', 'day', 'morning']\t['Each', 'day', 'I', 'run']\n",
      "1.1626442679190636\n",
      "\n",
      "['I', 'run', 'every', 'day', 'morning']\t['Each', 'I', 'run']\n",
      "1.804157856150221\n",
      "\n",
      "['I', 'run', 'every', 'day', 'morning']\t['Each', 'day', 'run']\n",
      "1.7262934209024152\n",
      "\n",
      "['I', 'run', 'every', 'day', 'morning']\t['Each', 'day', 'I']\n",
      "1.716070718874115\n",
      "\n",
      "['I', 'every', 'day', 'morning']\t['Each', 'day', 'I', 'run']\n",
      "1.4545022893238069\n",
      "\n",
      "['I', 'run', 'day', 'morning']\t['Each', 'day', 'I', 'run']\n",
      "1.0145831108093262\n",
      "\n",
      "['I', 'run', 'every', 'morning']\t['Each', 'day', 'I', 'run']\n",
      "1.1818685887026787\n",
      "\n",
      "['I', 'run', 'every']\t['Each', 'day', 'I', 'run']\n",
      "1.3422707755860321\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_distance('I run every day in the morning', 'Each day I run')\n",
    "sentence_distance('I run every day in the morning', 'Each I run')\n",
    "sentence_distance('I run every day in the morning', 'Each day run')\n",
    "sentence_distance('I run every day in the morning', 'Each day I')\n",
    "sentence_distance('I every day in the morning', 'Each day I run')\n",
    "sentence_distance('I run day in the morning', 'Each day I run')\n",
    "sentence_distance('I run every in morning', 'Each day I run')\n",
    "sentence_distance('I run every in', 'Each day I run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vect(w):\n",
    "    try:\n",
    "        return model.word_vec(w)\n",
    "    except KeyError:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "def calc_avg(s):\n",
    "    ws = [get_vect(w) for w in s.split() if w not in stopwords]\n",
    "    avg_vect = sum(ws)/len(ws)\n",
    "    return avg_vect\n",
    "\n",
    "\n",
    "from scipy.spatial import distance\n",
    "def get_euclidean(s1, s2):\n",
    "    return distance.euclidean(calc_avg(s1), calc_avg(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Astrology:', 'I', 'Capricorn', 'Sun', 'Cap', 'moon', 'cap', 'rising...what', 'say', 'me?']\t[\"I'm\", 'triple', 'Capricorn', '(Sun,', 'Moon', 'ascendant', 'Capricorn)', 'What', 'say', 'me?']\n",
      "2.49434997539555\n",
      "\n",
      "0.8109230127174104\n"
     ]
    }
   ],
   "source": [
    "# same questions\n",
    "s1 = 'Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?'\n",
    "s2 = \"I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\"\n",
    "sentence_distance(s1, s2)\n",
    "print(get_euclidean(s1, s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Astrology', 'I', 'Capricorn', 'Sun', 'Cap', 'moon', 'cap', 'rising', 'say']\t['I', 'triple', 'Capricorn', 'Sun', 'Moon', 'ascendant', 'Capricorn', 'What', 'say']\n",
      "2.0696045677228887\n",
      "\n",
      "0.9102963209152222\n"
     ]
    }
   ],
   "source": [
    "# same questions as above without punctuations\n",
    "s1 = 'Astrology I am a Capricorn Sun Cap moon and cap rising what does that say about me'\n",
    "s2 = \"I am a triple Capricorn Sun Moon and ascendant in Capricorn What does this say about me\"\n",
    "sentence_distance(s1, s2)\n",
    "print(get_euclidean(s1, s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'best', 'way', 'make', 'money', 'online']\t['What', 'best', 'way', 'ask', 'money', 'online?']\n",
      "0.9525941046914722\n",
      "\n",
      "0.5846541179497374\n"
     ]
    }
   ],
   "source": [
    "# same questions\n",
    "s1 = 'What is best way to make money online'\n",
    "s2 = 'What is best way to ask for money online?'\n",
    "sentence_distance(s1,s2)\n",
    "print(get_euclidean(s1, s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'Darth', 'Vader', 'fought', 'Darth', 'Maul', 'Star', 'Wars', 'Legends?']\t['Does', 'Quora', 'character', 'limit', 'profile', 'descriptions?']\n",
      "4.066483587027646\n",
      "\n",
      "1.7708145158159805\n"
     ]
    }
   ],
   "source": [
    "# different questions\n",
    "s1 = 'How did Darth Vader fought Darth Maul in Star Wars Legends?'\n",
    "s2 = 'Does Quora have a character limit for profile descriptions?'\n",
    "sentence_distance(s1,s2)\n",
    "print(get_euclidean(s1, s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.066483587027646\n",
      "4.066483587027646\n",
      "4.066483587027646\n",
      "4.066483587027646\n"
     ]
    }
   ],
   "source": [
    "# the order of the words doesn't change the distanace bewteeen the two phrases\n",
    "s1ws = [w for w in s1.split() if w not in stopwords]\n",
    "s2ws = [w for w in s2.split() if w not in stopwords]\n",
    "print(model.wmdistance(s1ws, s2ws) )\n",
    "print(model.wmdistance(s1ws[::-1], s2ws) )\n",
    "print(model.wmdistance(s1ws, s2ws[::-1]) )\n",
    "print(model.wmdistance(s1ws[3:]+s1ws[0:3], s2ws[::-1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### riflessioni:\n",
    "- le distanze medie funzionicchiano bene +-\n",
    "- non tengono conto dell'ordine con cui vengono scritte le parole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
