{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Install-pyspark-(and-make-it-run-on-jupyter-notebook-and-the-anaconda-python):\" data-toc-modified-id=\"Install-pyspark-(and-make-it-run-on-jupyter-notebook-and-the-anaconda-python):-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Install pyspark (and make it run on jupyter-notebook and the anaconda python):</a></div><div class=\"lev2 toc-item\"><a href=\"#Spark-Transformations:\" data-toc-modified-id=\"Spark-Transformations:-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Spark Transformations:</a></div><div class=\"lev2 toc-item\"><a href=\"#Spark-Actions:\" data-toc-modified-id=\"Spark-Actions:-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Spark Actions:</a></div><div class=\"lev1 toc-item\"><a href=\"#Example-using-the-user-of-the-moviedb\" data-toc-modified-id=\"Example-using-the-user-of-the-moviedb-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Example using the user of the moviedb</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install pyspark (and make it run on jupyter-notebook and the anaconda python):\n",
    "- go to http://spark.apache.org/downloads.html and download the spark version you need (the final directory is your SPARK_HOME)\n",
    "- do python setup.py install inside the python directory\n",
    "- remember you should have scala and java installed I think\n",
    "- export SPARK_HOME=\"/home/ale/Documents/mylibraries/spark-2.1.0-bin-hadoop2.7\"                       \n",
    "  export PATH=\"\\$SPARK_HOME/bin:\\$PATH\"                                                                 \n",
    "  export PYSPARK_DRIVER_PYTHON=jupyter                                                                \n",
    "  export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"                                                        \n",
    "  export PYSPARK_PYTHON=/home/ale/anaconda3/bin/python                                                \n",
    "- start pyspark (the exe is in the SPARK_HOME/bin directory) from a terminal and it will open a jupyter notebook with the python of anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check if spark is running (check cpu in htop can help too)\n",
    "sc._jsc.sc().isStopped()\n",
    "\n",
    "# To stop spark, refresh the jupyter or\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.141516\n",
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# calculate pi value in pyspark\n",
    "import random\n",
    "NUM_SAMPLES = 100000000\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)).filter(inside).count()\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.141539\n",
      "CPU times: user 37.2 s, sys: 652 ms, total: 37.8 s\n",
      "Wall time: 37.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# same as above in python only\n",
    "j = 0\n",
    "count = len([j for j in range(0, NUM_SAMPLES) if inside(j)])\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Transformations:\n",
    "- map()\n",
    "- flatMap()\n",
    "- filter()\n",
    "- sample()\n",
    "- union()\n",
    "- intersection()\n",
    "- distinct()\n",
    "- join()\n",
    "\n",
    "## Spark Actions:\n",
    "- reduce()\n",
    "- collect()\n",
    "- count()\n",
    "- first()\n",
    "- takeSample(withReplacement, num, [seed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusedRDD = sc.textFile(\"confused.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Confusion is the inability to think as clearly or quickly as you normally do.',\n",
       " '',\n",
       " 'You may  have difficulty paying attention to anything , remembering anyone, and making decisions.',\n",
       " '']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusedRDD.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# many of the function are the same as for the scala collections (map, flatmap, filter)\n",
    "mapRDD = confusedRDD.map(lambda x: x.split(' '))\n",
    "flatmapRDD = confusedRDD.flatMap(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Confusion',\n",
       "  'is',\n",
       "  'the',\n",
       "  'inability',\n",
       "  'to',\n",
       "  'think',\n",
       "  'as',\n",
       "  'clearly',\n",
       "  'or',\n",
       "  'quickly',\n",
       "  'as',\n",
       "  'you',\n",
       "  'normally',\n",
       "  'do.'],\n",
       " [''],\n",
       " ['You',\n",
       "  'may',\n",
       "  '',\n",
       "  'have',\n",
       "  'difficulty',\n",
       "  'paying',\n",
       "  'attention',\n",
       "  'to',\n",
       "  'anything',\n",
       "  ',',\n",
       "  'remembering',\n",
       "  'anyone,',\n",
       "  'and',\n",
       "  'making',\n",
       "  'decisions.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Confusion', 'is', 'the']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatmapRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onlyconfusion = confusedRDD.filter(lambda x : (\"confus\" in x.lower()))\n",
    "onlyconfusion.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Confusion is the inability to think as clearly or quickly as you normally do.',\n",
       " 'Confusion may come to anyone early or late phase of the life, depending on the reason behind it .',\n",
       " 'Many times, confusion lasts for a very short span and goes away.',\n",
       " 'Confusion is more common in people who are in late stages of the life and often occurs when you have stayed in hospital.',\n",
       " 'Some confused people may have strange or unusual behavior or may act aggressively.',\n",
       " 'A good way to find out if anyone is confused is to question the person their identity i.e. name, age, and the date.',\n",
       " 'If they are little not sure or unable to answer correctly, they are confused']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onlyconfusion.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " 'Other times, it may be permanent and has no cure. It may have association with delirium or dementia.',\n",
       " 'Confusion is more common in people who are in late stages of the life and often occurs when you have stayed in hospital.',\n",
       " '',\n",
       " 'Some confused people may have strange or unusual behavior or may act aggressively.',\n",
       " 'A good way to find out if anyone is confused is to question the person their identity i.e. name, age, and the date.',\n",
       " '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This transformation is used to pick sample RDD from a larger RDD. \n",
    "# It is frequently used in Machine learning operations where a sample of the dataset needs to be taken. \n",
    "# The fraction means percentage of the total data you want to take the sample from.\n",
    "sampledconfusion = confusedRDD.sample(False,0.5,4)  # True implies withReplacement\n",
    "sampledconfusion.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('physics', 85),\n",
       " ('maths', 75),\n",
       " ('chemistry', 95),\n",
       " ('physics', 65),\n",
       " ('maths', 45),\n",
       " ('chemistry', 85)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Union is basically used to merge two RDDs together if they have the same structure. \n",
    "# (here the data is a pair string-int)\n",
    "abhay_marks = [(\"physics\",85),(\"maths\",75),(\"chemistry\",95)]\n",
    "ankur_marks = [(\"physics\",65),(\"maths\",45),(\"chemistry\",85)]\n",
    "\n",
    "abhay = sc.parallelize(abhay_marks)\n",
    "ankur = sc.parallelize(ankur_marks)\n",
    "\n",
    "abhay.union(ankur).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('physics', (85, 65)), ('maths', (75, 45)), ('chemistry', (95, 85))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This transformation joins two RDDs based on a common key.\n",
    "Subject_wise_marks = abhay.join(ankur)\n",
    "Subject_wise_marks.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rahul', 'abhay']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intersection gives you the common terms or objects from the two RDDS.\n",
    "Cricket_team = [\"sachin\",\"abhay\",\"michael\",\"rahane\",\"david\",\"ross\",\"raj\",\"rahul\",\"hussy\",\"steven\",\"sourav\"]\n",
    "Toppers = [\"rahul\",\"abhay\",\"laxman\",\"bill\",\"steve\"]\n",
    "\n",
    "cricketRDD = sc.parallelize(Cricket_team)\n",
    "toppersRDD = sc.parallelize(Toppers)\n",
    "\n",
    "toppercricketers = toppersRDD.intersection(cricketRDD)\n",
    "toppercricketers.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie1', 'movie3', 'movie7', 'movie5', 'movie8', 'movie11', 'movie1', 'movie5', 'movie10', 'movie7', 'movie10', 'movie4', 'movie6', 'movie7', 'movie3']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['movie10',\n",
       " 'movie8',\n",
       " 'movie5',\n",
       " 'movie1',\n",
       " 'movie4',\n",
       " 'movie7',\n",
       " 'movie3',\n",
       " 'movie11',\n",
       " 'movie6']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This transformation is used to get rid of any ambiguities.\n",
    "# As the name suggest it picks out the lines from the RDD that are unique.\n",
    "best_story = [\"movie1\",\"movie3\",\"movie7\",\"movie5\",\"movie8\"]\n",
    "best_direction = [\"movie11\",\"movie1\",\"movie5\",\"movie10\",\"movie7\"]\n",
    "best_screenplay = [\"movie10\",\"movie4\",\"movie6\",\"movie7\",\"movie3\"]\n",
    "\n",
    "story_rdd = sc.parallelize(best_story)\n",
    "direction_rdd = sc.parallelize(best_direction)\n",
    "screen_rdd = sc.parallelize(best_screenplay)\n",
    "\n",
    "total_nomination_rdd = story_rdd.union(direction_rdd).union(screen_rdd)\n",
    "print(total_nomination_rdd.collect())\n",
    "\n",
    "unique_movies_rdd = total_nomination_rdd.distinct()\n",
    "unique_movies_rdd .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Parallelism is the key feature of any distributed system where operations are done by dividing the data into multiple parallel partitions. The same operation is performed on the partitions simultaneously which helps achieve fast data processing with spark. Map and Reduce operations can be effectively applied in parallel in apache spark by dividing the data into multiple partitions. A copy of each partition within an RDD is distributed across several workers running on different nodes of a cluster so that in case of failure of a single worker the RDD still remains available. ***\n",
    "\n",
    "***  Degree of parallelism of each operation on RDD depends on the fixed number of partitions that an RDD has. We can specify the degree of parallelism or the number of partitions when creating it or later on using the repartition () and coalesce() methods. ***\n",
    "\n",
    "*** coalesce ()  is an optimized version of repartition() method that avoids data movement and is generally used to decrease the number of partitions after filtering a large dataset. ***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"/home/ale/.steam/steam/steamapps/common/Pillars\\ of\\ Eternity/PillarsOfEternity_Data/data/conversations/10_od_nua/1002_cv_pet_wurm.conversation\"\n",
    "partRDD = sc.textFile(filename,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** When processing data with reduceByKey operation, Spark will form as many number of output partitions based on the default parallelism which depends on the numbers of nodes and cores available on each node. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This runs a map operation individually on each partition unlike a normal map operation \n",
    "# where map is used to operate on each line of the entire RDD.\n",
    "partRDD.mapPartitions() \n",
    "\n",
    "# This works same as partRDD.mapPartitions but we can additionally \n",
    "# specify the partition number on which this operation has to be applied.\n",
    "mapPartitionsWithIndex() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Accumulators in spark are the global variable that can be shared across tasks. The scope of normal variables is just limited to a specific task ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example using the user of the moviedb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userRDD = sc.textFile(\"./ml-100k/u.user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1|24|M|technician|85711',\n",
       " '2|53|F|other|94043',\n",
       " '3|23|M|writer|32067',\n",
       " '4|24|M|technician|43537',\n",
       " '5|33|F|other|15213',\n",
       " '6|42|M|executive|98101',\n",
       " '7|57|M|administrator|91344',\n",
       " '8|36|M|administrator|05201',\n",
       " '9|29|M|student|01002',\n",
       " '10|53|M|lawyer|90703']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "def parse_N_calculate_age(data):\n",
    "    userid, age, gender, occupation, zip = data.split(\"|\")\n",
    "    return userid, age_group(int(age)), gender, occupation, zip, int(age)\n",
    "\n",
    "\n",
    "def age_group(age):\n",
    "    if age < 10:\n",
    "        return '0-10'\n",
    "    elif age < 20:\n",
    "        return '10-20'\n",
    "    elif age < 30:\n",
    "        return '20-30'\n",
    "    elif age < 40:\n",
    "        return '30-40'\n",
    "    elif age < 50:\n",
    "        return '40-50'\n",
    "    elif age < 60:\n",
    "        return '50-60'\n",
    "    elif age < 70:\n",
    "        return '60-70'\n",
    "    elif age < 80:\n",
    "        return '70-80'\n",
    "    else:\n",
    "        return '80+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_with_age_bucket = userRDD.map(parse_N_calculate_age)\n",
    "RDD_20_30 = data_with_age_bucket.filter(lambda line : '20-30' in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'administrator': 19,\n",
       " 'artist': 12,\n",
       " 'doctor': 2,\n",
       " 'educator': 12,\n",
       " 'engineer': 23,\n",
       " 'entertainment': 8,\n",
       " 'executive': 7,\n",
       " 'healthcare': 4,\n",
       " 'homemaker': 3,\n",
       " 'lawyer': 4,\n",
       " 'librarian': 11,\n",
       " 'marketing': 5,\n",
       " 'none': 2,\n",
       " 'other': 38,\n",
       " 'programmer': 30,\n",
       " 'salesman': 2,\n",
       " 'scientist': 8,\n",
       " 'student': 116,\n",
       " 'technician': 12,\n",
       " 'writer': 14}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = RDD_20_30.map(lambda line : line[3]).countByValue()\n",
    "dict(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[63] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we are done with the operations on the above cached data we can remove them from memory using unpersisit () \n",
    "RDD_20_30.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Under_age = sc.accumulator(0)\n",
    "Over_age = sc.accumulator(0)\n",
    "\n",
    "def outliers(data):\n",
    "    global Over_age, Under_age\n",
    "    age_grp = data[1]\n",
    "    if(age_grp == \"70-80\"):\n",
    "        Over_age +=1\n",
    "    if(age_grp == \"0-10\"):\n",
    "        Under_age +=1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = data_with_age_bucket.map(outliers).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=0, value=1>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Under_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=1, value=4>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Over_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** To Running a Spark application in Standalone Mode** \n",
    "\n",
    "- create a file .py where all the functions and commands are stored and then run:\n",
    "- spark-submit myfile.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "100px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
