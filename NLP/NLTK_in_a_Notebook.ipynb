{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev3 toc-item\"><a href=\"#Useful-link-for-italian-lenguage-support:\" data-toc-modified-id=\"Useful-link-for-italian-lenguage-support:-001\"><span class=\"toc-item-num\">0.0.1&nbsp;&nbsp;</span>Useful link for italian lenguage support:</a></div><div class=\"lev1 toc-item\"><a href=\"#Tokenize:-sentences-and-words\" data-toc-modified-id=\"Tokenize:-sentences-and-words-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Tokenize: sentences and words</a></div><div class=\"lev1 toc-item\"><a href=\"#POS-Tagging\" data-toc-modified-id=\"POS-Tagging-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>POS Tagging</a></div><div class=\"lev1 toc-item\"><a href=\"#Stemming:\" data-toc-modified-id=\"Stemming:-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Stemming:</a></div><div class=\"lev3 toc-item\"><a href=\"#riduzione-di-una-parola-alla-sua-radice-o-forma-base\" data-toc-modified-id=\"riduzione-di-una-parola-alla-sua-radice-o-forma-base-301\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>riduzione di una parola alla sua radice o forma base</a></div><div class=\"lev1 toc-item\"><a href=\"#Lemmization:\" data-toc-modified-id=\"Lemmization:-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Lemmization:</a></div><div class=\"lev3 toc-item\"><a href=\"#riduzione-al-lemma:-considera-anche-il-contesto-della-parola\" data-toc-modified-id=\"riduzione-al-lemma:-considera-anche-il-contesto-della-parola-401\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>riduzione al lemma: considera anche il contesto della parola</a></div><div class=\"lev1 toc-item\"><a href=\"#Italian-Tagging-and-Lemming\" data-toc-modified-id=\"Italian-Tagging-and-Lemming-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Italian Tagging and Lemming</a></div><div class=\"lev2 toc-item\"><a href=\"#http://sslmitdev-online.sslmit.unibo.it/linguistics/morph-it.php\" data-toc-modified-id=\"http://sslmitdev-online.sslmit.unibo.it/linguistics/morph-it.php-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span><a href=\"http://sslmitdev-online.sslmit.unibo.it/linguistics/morph-it.php\" target=\"_blank\">http://sslmitdev-online.sslmit.unibo.it/linguistics/morph-it.php</a></a></div><div class=\"lev1 toc-item\"><a href=\"#STANFORD\" data-toc-modified-id=\"STANFORD-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>STANFORD</a></div><div class=\"lev3 toc-item\"><a href=\"#POSTagger\" data-toc-modified-id=\"POSTagger-601\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;</span>POSTagger</a></div><div class=\"lev3 toc-item\"><a href=\"#Named-Entity-Recognizer-(NER)\" data-toc-modified-id=\"Named-Entity-Recognizer-(NER)-602\"><span class=\"toc-item-num\">6.0.2&nbsp;&nbsp;</span>Named Entity Recognizer (NER)</a></div><div class=\"lev3 toc-item\"><a href=\"#Parser:-analisi-logica\" data-toc-modified-id=\"Parser:-analisi-logica-603\"><span class=\"toc-item-num\">6.0.3&nbsp;&nbsp;</span>Parser: analisi logica</a></div><div class=\"lev1 toc-item\"><a href=\"#Classification-algorithms:\" data-toc-modified-id=\"Classification-algorithms:-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Classification algorithms:</a></div><div class=\"lev2 toc-item\"><a href=\"#Assegnazioni-di-classi-o-categorie-al-testo\" data-toc-modified-id=\"Assegnazioni-di-classi-o-categorie-al-testo-71\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Assegnazioni di classi o categorie al testo</a></div><div class=\"lev3 toc-item\"><a href=\"#Classic-models:\" data-toc-modified-id=\"Classic-models:-711\"><span class=\"toc-item-num\">7.1.1&nbsp;&nbsp;</span>Classic models:</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful link for italian lenguage support:\n",
    "- http://linguistica.sns.it/CoLFIS/Home_eng.htm   lemmi con frequenze da quotidiani, articoli etc...\n",
    "- http://www.clips.ua.ac.be/pages/pattern-it    pattern ma è in python 2.6 \n",
    "- http://badip.uni-graz.at/it/   altra banca dati di vocabolario (non troppo utile)\n",
    "- nltk tools (vedi link sotto)\n",
    "- http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/   utile per fare il tag delle parole in italiano\n",
    "- https://github.com/miotto/treetagger-python   per usare TreeTagger in python (non riuscito a compilarlo in windows)\n",
    "- morphit (grosso vocabolario italiano mancano le frequenze, vedi link sotto)\n",
    "- https://github.com/napolux/paroleitaliane (altre parole italiane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from math import *\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize: sentences and words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract sentences\n",
    "sents = nltk.sent_tokenize(text)\n",
    "\n",
    "#extract words\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# tagged tokens\n",
    "tagged_tokens = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Come ormai tutti sanno, ogni anno, in gran parte del mondo, si celebra il Giorno della Memoria (27 gennaio), in memoria dei terribili crimini contro l’umanità perpetrati dai Nazisti prima e durante la Seconda Guerra Mondiale.',\n",
       " 'Milioni di ebrei furono deportati nei campi di concentramento e sterminio; i più fortunati riuscirono a nascondersi o fuggire prima.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentences in another language:\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/italian.pickle')\n",
    "\n",
    "italiantext = 'Come ormai tutti sanno, ogni anno, in gran parte del mondo, si celebra il Giorno della Memoria (27 gennaio), in memoria dei terribili crimini contro l’umanità perpetrati dai Nazisti prima e durante la Seconda Guerra Mondiale. Milioni di ebrei furono deportati nei campi di concentramento e sterminio; i più fortunati riuscirono a nascondersi o fuggire prima.'\n",
    "\n",
    "tokenizer.tokenize(italiantext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n"
     ]
    }
   ],
   "source": [
    "# POS tagging\n",
    "nltk.help.upenn_tagset('RB')\n",
    "nltk.help.upenn_tagset('NN.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Non c'è un solo pos tagger...quello di default è il maxent treebank pos tagger, ma ci sono anche\n",
    "# il crf,hmm,brill,tnt\n",
    "\n",
    "# QUALI SONO LE DIFFERENZE??\n",
    "\n",
    "# Trainare un tagger:\n",
    "from nltk.corpus import treebank\n",
    "train_data = treebank.tagged_sents()[:3000]\n",
    "test_data = treebank.tagged_sents()[3000:]\n",
    "\n",
    "from nltk.tag import tnt\n",
    "tnt_pos_tagger = tnt.TnT()\n",
    "tnt_pos_tagger.train(train_data)\n",
    "tnt_pos_tagger.evaluate(test_data)\n",
    "\n",
    "import pickle\n",
    "f = open(‘tnt_treebank_pos_tagger.pickle’, ‘w’)\n",
    "pickle.dump(tnt_pos_tagger, f)\n",
    "f.close()\n",
    "\n",
    "tnt_tagger.tag(nltk.word_tokenize(“this is a tnt treebank tnt tagger”))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming: \n",
    "### riduzione di una parola alla sua radice o forma base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "# https://tartarus.org/martin/PorterStemmer/\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "# http://snowball.tartarus.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'say'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "porter_stemmer.stem('saying')\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "lancaster_stemmer.stem('saying')\n",
    "\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "snowball_stemmer.stem('saying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parl'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemmer_it = SnowballStemmer('italian')\n",
    "snowball_stemmer_it.stem('parlando')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmization:\n",
    "### riduzione al lemma: considera anche il contesto della parola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://wordnet.princeton.edu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('are')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('is', pos='v')\n",
    "# risulta quindi importante fare pos tagging prima della lemmization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian Tagging and Lemming\n",
    "## http://sslmitdev-online.sslmit.unibo.it/linguistics/morph-it.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STANFORD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_postagger = StanfordPOSTagger('C:\\\\stanford-postagger-full-2014-08-27\\\\models\\\\english-bidirectional-distsim.tagger',\n",
    "                                      'C:\\\\stanford-postagger-full-2014-08-27\\\\stanford-postagger.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('stanford', 'JJ'),\n",
       " ('postagger', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('nltk', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('python', 'NN'),\n",
       " ('users', 'NNS')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_postagger.tag('this is stanford postagger in nltk for python users'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognizer (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-ner.jar jar file at C:\\stanford-ner-2014-08-27\\stanford-ner.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bd33fac66288>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m english_nertagger = StanfordNERTagger('C:\\\\stanford-ner-2014-08-27\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz',\n\u001b[0;32m----> 2\u001b[0;31m                               'C:\\\\stanford-ner-2014-08-27\\\\stanford-ner.jar')\n\u001b[0m",
      "\u001b[0;32m/home/ale/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStanfordNERTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ale/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_JAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0msearchpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 verbose=verbose)\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         self._stanford_model = find_file(model_filename,\n",
      "\u001b[0;32m/home/ale/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    717\u001b[0m         searchpath=(), url=None, verbose=True, is_regex=False):\n\u001b[1;32m    718\u001b[0m     return next(find_jar_iter(name_pattern, path_to_jar, env_vars,\n\u001b[0;32m--> 719\u001b[0;31m                          searchpath, url, verbose, is_regex))\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ale/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             raise LookupError('Could not find %s jar file at %s' %\n\u001b[0;32m--> 635\u001b[0;31m                             (name_pattern, path_to_jar))\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;31m# Check environment variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-ner.jar jar file at C:\\stanford-ner-2014-08-27\\stanford-ner.jar"
     ]
    }
   ],
   "source": [
    "english_nertagger = StanfordNERTagger('C:\\\\stanford-ner-2014-08-27\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz',\n",
    "                              'C:\\\\stanford-ner-2014-08-27\\\\stanford-ner.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_nertagger.tag('Pincopallino is working at StarWars in Montevarchi'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser: analisi logica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_parser = StanfordParser('C:\\\\stanford-parser-full-2014-08-27\\\\stanford-parser.jar',\n",
    "                                'C:\\\\stanford-parser-full-2014-08-27\\\\stanford-parser-3.4.1-models.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analisi = english_parser.parse_sents('Francesco is a good guy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification algorithms:\n",
    "## Assegnazioni di classi o categorie al testo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic models:\n",
    "    - Naive Bayes Model\n",
    "    - Maximum Entropy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T12:11:22.805511Z",
     "start_time": "2017-08-05T12:11:22.790286Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "import collections\n",
    "\n",
    "\n",
    "names = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])\n",
    " \n",
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T12:50:17.408787Z",
     "start_time": "2017-08-05T12:50:17.326536Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    word = word.lower()\n",
    "    most_freq_char = collections.Counter(word).most_common(1)[0]\n",
    "    repeated_char = [word[i] for i in range(0,len(word)-1) if word[i+1] == word[i]]\n",
    "    VOWELS = 'aeiou'\n",
    "    return {\n",
    "        'last_letter': word[-1],\n",
    "#         'last_two_letters': word[-2:],\n",
    "#         'first_letters': word[0],\n",
    "#         'number_of_k': len([i for i in word if i == 'k']),\n",
    "#         'number_of_e': len([i for i in word if i == 'e']),\n",
    "#         'most_freq_char': most_freq_char[0] if most_freq_char[1] > 1 else '',\n",
    "        'repeated_char': '' if len(repeated_char) == 0 else repeated_char[-1],\n",
    "#         'start_vowels': word[0] in VOWELS,\n",
    "#          'end_vowels': word[-1] in VOWELS,        \n",
    "#          'num_vowels': len([i for i in word if i in VOWELS]),\n",
    "#          'num_non_vowels': len([i for i in word if i not in VOWELS])\n",
    "    }\n",
    "\n",
    "\n",
    "featuresets = [(gender_features(n), g) for (n, g) in names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T12:50:17.796655Z",
     "start_time": "2017-08-05T12:50:17.748657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802\n",
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     35.5 : 1.0\n",
      "             last_letter = 'k'              male : female =     30.3 : 1.0\n",
      "             last_letter = 'f'              male : female =     15.9 : 1.0\n",
      "             last_letter = 'p'              male : female =     11.9 : 1.0\n",
      "             last_letter = 'v'              male : female =     11.2 : 1.0\n",
      "             last_letter = 'd'              male : female =     10.3 : 1.0\n",
      "             last_letter = 'm'              male : female =      8.7 : 1.0\n",
      "             last_letter = 'o'              male : female =      8.1 : 1.0\n",
      "             last_letter = 'r'              male : female =      6.5 : 1.0\n",
      "             last_letter = 'g'              male : female =      5.4 : 1.0\n",
      "             last_letter = 'w'              male : female =      5.4 : 1.0\n",
      "           repeated_char = 'o'              male : female =      5.1 : 1.0\n",
      "           repeated_char = 'a'              male : female =      5.1 : 1.0\n",
      "             last_letter = 't'              male : female =      4.4 : 1.0\n",
      "             last_letter = 's'              male : female =      4.3 : 1.0\n",
      "             last_letter = 'z'              male : female =      4.0 : 1.0\n",
      "             last_letter = 'j'              male : female =      4.0 : 1.0\n",
      "             last_letter = 'b'              male : female =      3.8 : 1.0\n",
      "             last_letter = 'i'            female : male   =      3.8 : 1.0\n",
      "           repeated_char = 'e'            female : male   =      3.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(nb_classifier, test_set))\n",
    "\n",
    "nb_classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T13:31:26.740221Z",
     "start_time": "2017-08-05T13:30:49.671307Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.371\n",
      "             2          -0.43404        0.749\n",
      "             3          -0.38942        0.772\n",
      "             4          -0.37293        0.772\n",
      "             5          -0.36564        0.773\n",
      "             6          -0.36210        0.773\n",
      "             7          -0.36027        0.773\n",
      "             8          -0.35928        0.773\n",
      "             9          -0.35873        0.773\n",
      "            10          -0.35840        0.773\n",
      "            11          -0.35820        0.773\n",
      "            12          -0.35807        0.773\n",
      "            13          -0.35798        0.773\n",
      "            14          -0.35791        0.773\n",
      "            15          -0.35786        0.773\n",
      "            16          -0.35782        0.773\n",
      "            17          -0.35779        0.773\n",
      "            18          -0.35776        0.773\n",
      "            19          -0.35773        0.773\n",
      "            20          -0.35771        0.773\n",
      "            21          -0.35769        0.773\n",
      "            22          -0.35767        0.773\n",
      "            23          -0.35765        0.773\n",
      "            24          -0.35764        0.773\n",
      "            25          -0.35763        0.773\n",
      "            26          -0.35761        0.773\n",
      "            27          -0.35760        0.773\n",
      "            28          -0.35759        0.773\n",
      "            29          -0.35758        0.773\n",
      "            30          -0.35757        0.773\n",
      "            31          -0.35756        0.773\n",
      "            32          -0.35755        0.773\n",
      "            33          -0.35754        0.773\n",
      "            34          -0.35753        0.773\n",
      "            35          -0.35753        0.773\n",
      "            36          -0.35752        0.773\n",
      "            37          -0.35751        0.773\n",
      "            38          -0.35751        0.773\n",
      "            39          -0.35750        0.773\n",
      "            40          -0.35749        0.773\n",
      "            41          -0.35749        0.773\n",
      "            42          -0.35748        0.773\n",
      "            43          -0.35748        0.773\n",
      "            44          -0.35747        0.773\n",
      "            45          -0.35747        0.773\n",
      "            46          -0.35746        0.773\n",
      "            47          -0.35746        0.773\n",
      "            48          -0.35746        0.773\n",
      "            49          -0.35745        0.773\n",
      "            50          -0.35745        0.773\n",
      "            51          -0.35744        0.773\n",
      "            52          -0.35744        0.773\n",
      "            53          -0.35744        0.773\n",
      "            54          -0.35743        0.773\n",
      "            55          -0.35743        0.773\n",
      "            56          -0.35743        0.773\n",
      "            57          -0.35742        0.773\n",
      "            58          -0.35742        0.773\n",
      "            59          -0.35742        0.773\n",
      "            60          -0.35742        0.773\n",
      "            61          -0.35741        0.773\n",
      "            62          -0.35741        0.773\n",
      "            63          -0.35741        0.773\n",
      "            64          -0.35741        0.773\n",
      "            65          -0.35740        0.773\n",
      "            66          -0.35740        0.773\n",
      "            67          -0.35740        0.773\n",
      "            68          -0.35740        0.773\n",
      "            69          -0.35740        0.773\n",
      "            70          -0.35739        0.773\n",
      "            71          -0.35739        0.773\n",
      "            72          -0.35739        0.773\n",
      "            73          -0.35739        0.773\n",
      "            74          -0.35739        0.773\n",
      "            75          -0.35738        0.773\n",
      "            76          -0.35738        0.773\n",
      "            77          -0.35738        0.773\n",
      "            78          -0.35738        0.773\n",
      "            79          -0.35738        0.773\n",
      "            80          -0.35738        0.773\n",
      "            81          -0.35738        0.773\n",
      "            82          -0.35737        0.773\n",
      "            83          -0.35737        0.773\n",
      "            84          -0.35737        0.773\n",
      "            85          -0.35737        0.773\n",
      "            86          -0.35737        0.773\n",
      "            87          -0.35737        0.773\n",
      "            88          -0.35737        0.773\n",
      "            89          -0.35736        0.773\n",
      "            90          -0.35736        0.773\n",
      "            91          -0.35736        0.773\n",
      "            92          -0.35736        0.773\n",
      "            93          -0.35736        0.773\n",
      "            94          -0.35736        0.773\n",
      "            95          -0.35736        0.773\n",
      "            96          -0.35736        0.773\n",
      "            97          -0.35736        0.773\n",
      "            98          -0.35735        0.773\n",
      "            99          -0.35735        0.773\n",
      "         Final          -0.35735        0.773\n",
      "0.802\n",
      "   5.661 last_letter=='c' and label is 'male'\n",
      "   5.556 last_letter==' ' and label is 'female'\n",
      "   5.170 repeated_char=='v' and label is 'female'\n",
      "  -5.141 last_letter=='a' and label is 'male'\n",
      "  -3.698 last_letter=='k' and label is 'female'\n"
     ]
    }
   ],
   "source": [
    "# Maxent Classifier\n",
    "\n",
    "me_classifier = nltk.MaxentClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(me_classifier, test_set))\n",
    "\n",
    "me_classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.373\n",
      "             2          -0.61404        0.627\n",
      "             3          -0.59980        0.627\n",
      "             4          -0.58638        0.628\n",
      "             5          -0.57376        0.636\n",
      "             6          -0.56189        0.653\n",
      "             7          -0.55076        0.672\n",
      "      Training stopped: keyboard interrupt\n",
      "         Final          -0.54031        0.688\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(gender_features2(n), g) for (n, g) in names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "\n",
    "nb2_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "me2_classifier = nltk.MaxentClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.766\n",
      "0.714\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(nb2_classifier, test_set))\n",
    "print(nltk.classify.accuracy(me2_classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "423px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
